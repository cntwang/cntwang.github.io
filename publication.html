---
layout: post
title: "Publications"
date:       2025-01	
author:     Ruihang Chu			
header-img: img/Transparent_1.jpg
---
<i><span style="color:red"> News:</span> <a href="https://tongyi.aliyun.com/wanxiang/videoCreation" style="color:rgb(51, 156, 255)">Wanx 2.1</a> has topped <a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard" style="color:rgb(51, 156, 255)">VBench</a> leaderboard, outperforming the latest renowned video generation models like Gen-3, Luma, Kling, and Minimax-Video.</i><br>
<i><span style="color:red"> News:</span> We release the paper, code, and models for <a href="https://arxiv.org/abs/2403.18814" style="color:rgb(51, 156, 255)">Mini-Gemini</a>.</i><br>
<i><span style="color:red"> News:</span> <a href="https://arxiv.org/abs/2306.16329" style="color:rgb(51, 156, 255)">DiffComplete</a> is accepted by <b>NeurIPS 2023</b>, a controllable, multi-modal, and high-fidelity shape completion method using diffusion models.</i><br>
<i><span style="color:red"> News:</span> <a href="https://arxiv.org/abs/2307.01831" style="color:rgb(51, 156, 255)">DiT-3D</a> is accepted by <b>NeurIPS 2023</b>, a Diffusion Transformer for 3D shape generation that is powerful and supports efficient fine-tuning from 2D to 3D.</i><br>
<i><span style="color:red"> News:</span> <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf" style="color:rgb(51, 156, 255)">Cart</a> is accepted by <b>CVPR 2023</b>, an approach towards articulated shape manipulation driven by simple command templates.</i><br>
<i><span style="color:red"> News:</span> <a href="https://arxiv.org/pdf/2303.16485.pdf" style="color:rgb(51, 156, 255)">TriVol</a> is accepted by <b>CVPR 2023</b>, a dense while lightweight 3D representation combined with <a href="https://www.matthewtancik.com/nerf" style="color:rgb(51, 156, 255)">NeRF</a> for rendering photorealistic images from point clouds.</i><br>
<i><span style="color:red"> News:</span> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chu_TWIST_Two-Way_Inter-Label_Self-Training_for_Semi-Supervised_3D_Instance_Segmentation_CVPR_2022_paper.pdf" style="color:rgb(51, 156, 255)">TWIST</a> is accepted by <b>CVPR 2022</b>, the first self-training framework for semi-supervised 3D instance segmentation.</i><br>
<!-- <i><span style="color:red"> News:</span> <a href="https://arxiv.org/pdf/2108.11771.pdf" style="color:rgb(51, 156, 255)">ICM-3D</a> is accepted by <b>RA-L 2021</b>, which solves 3D instance segmentation via a per-point classification paradigm.</i><br> -->
<!-- <i><span style="color:red"> News:</span> 1 paper accepted by <b>IROS 2021</b>, Prague.</i><br> -->
<!-- <i><span style="color:red"> News:</span> 1 paper accepted by <b>CVPR 2021</b>, Virtual.</i><br> -->
<hr>

See <a href="https://scholar.google.com.hk/citations?user=62zPPxkAAAAJ&hl=en" style="color:rgb(51, 156, 255)"><b>Google Scholar</b></a> for the full list.
<hr>

<h2> Preprints</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Mini_Gemini.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</span></h5>
            Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, <b>Ruihang Chu</b>, Shaoteng Liu, Jiaya Jia<br>
            <i>arXiv:2403.18814</i>
            <br>[<a href="https://arxiv.org/abs/2403.18814" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://mini-gemini.github.io/" style="color:rgb(51, 156, 255)">project page</a>] [<a href="https://github.com/dvlab-research/MiniGemini" style="color:rgb(51, 156, 255)">code</a>] [<a href="https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854" style="color:rgb(51, 156, 255)">models</a>] [<a href="https://huggingface.co/collections/YanweiLi/mgm-data-660463ea895a01d8f367624e" style="color:rgb(51, 156, 255)">data</a>] [<a href="http://103.170.5.190:7860/" style="color:rgb(51, 156, 255)">demo</a>] [<a href="https://www.jiqizhixin.com/articles/2024-04-16-5" style="color:rgb(51, 156, 255)">机器之心</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/freescale.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">FreeScale: Unleashing the Resolution of Diffusion Models ia Tuning-Free Scale Fusion</span></h5>
            Haonan Qiu, Shiwei Zhang, Yujie Wei, <b>Ruihang Chu</b>, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu<br>
            <i>arXiv:2412.09626</i>
            <br>[<a href="https://arxiv.org/abs/2412.09626" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/ali-vilab/FreeScale" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/layout.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">LayoutDiT: Exploring Content-Graphic Balance in Layout Generation with Diffusion Transformer</span></h5>
            Yu Li 1, Yifan Chen, Gongye Liu, Fei Yin, Qingyan Bai, Jie Wu, Hongfa Wang, <b>Ruihang Chu†</b>, Yujiu Yang†<br>
            <i>arXiv:2407.15233 (†Corresponding author)</i>
            <br>[<a href="https://arxiv.org/abs/2407.15233" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://yuli0103.github.io/LayoutDiT.github.io/" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DriveCoT.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving</span></h5>
            Tianqi Wang, Enze Xie, <b>Ruihang Chu</b>, Zhenguo Li, Ping Luo<br>
            <i>arXiv:2403.16996</i>
            <br>[<a href="https://arxiv.org/abs/2403.16996" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://drivecot.github.io/" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DialogGen.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation</span></h5>
            Minbin Huang, Yanxin Long, Xinchi Deng, <b>Ruihang Chu</b>, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, Wei Liu<br>
            <i>arXiv:2403.08857</i>
            <br>[<a href="https://arxiv.org/abs/2403.08857" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://hunyuan-dialoggen.github.io/" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Reasoning.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">A Survey of Reasoning with Foundation Models</span></h5>
            Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, <b>Ruihang Chu</b>, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li<br>
            <i>arXiv:2312.11562</i>
            <br>[<a href="https://arxiv.org/abs/2312.11562" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
</table>
<hr>

<h2> Journal Papers</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/ICM3D.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">ICM-3D: Instantiated Category Modeling for 3D Instance Segmentation</span></h5>
            <b>Ruihang Chu</b>, Yukang Chen, Lu Qi, Tao Kong, Lei Li<br>
            <i>IEEE Robotics and Automation Letters <b>(RA-L)</b>, 2021</i>
            <br>[<a href="https://arxiv.org/abs/2108.11771" style="color:rgb(51, 156, 255)">paper</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Co-actuation.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Co-actuation: A Method for Achieving High Stiﬀness and Low Inertia for Haptic Devices</span></h5>
            <b>Ruihang Chu</b>, Yuru Zhang, Hongdong Zhang, Weiliang Xu, Jee-Hwan Ryu, Dangxiao Wang<br>
            <i>IEEE Transactions on Haptics <b>(ToH)</b>, 2020</i>
            <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8859238/" style="color:rgb(51, 156, 255)">paper</a>]
    </td></tr>
</table>
<hr>

<h2> Conference Papers</h2>
<table frame=void rules=none>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/DiffComplete.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">DiffComplete: Diffusion-based Generative 3D Shape Completion</span></h5>
            <b>Ruihang Chu</b>, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, Jiaya Jia<br>
            <i>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2023</i> 
            <br>[<a href="https://arxiv.org/abs/2306.16329" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://ruihangchu.com/diffcomplete.html" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Cart.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Command-driven Articulated Object Understanding and Manipulation</span></h5>
            <b>Ruihang Chu</b>, Zhengzhe Liu, Xiaoqing Ye, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, Jiaya Jia
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2023</i> 
            <br>[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://www.youtube.com/watch?v=-UJBvLH93uM&t=30s" style="color:rgb(51, 156, 255)">video</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/TWIST.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">TWIST: Two-Way Inter-label Self-Training for Semi-supervised 3D Instance Segmentation</span></h5>
            <b>Ruihang Chu</b>, Xiaoqing Ye, Zhengzhe Liu, Xiao Tan, Xiaojuan Qi, Chi-Wing Fu, Jiaya Jia
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2022</i> 
            <br>[<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chu_TWIST_Two-Way_Inter-Label_Self-Training_for_Semi-Supervised_3D_Instance_Segmentation_CVPR_2022_paper.html" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/TWIST" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/VANet.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Vehicle Re-identiﬁcation with Viewpoint-aware Metric Learning</span></h5>
            <b>Ruihang Chu</b>, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang, Yichen Wei<br>
            <i>IEEE International Conference on Computer Vision <b>(ICCV)</b>, 2019</i> 
            <br>[<a href="https://arxiv.org/abs/1910.04104" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/emdata-ailab/VANET" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/dit3d.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation</span></h5>
            Shentong Mo, Enze Xie, <b>Ruihang Chu</b>, Lewei Yao, Lanqing Hong, Matthias Nießner, Zhenguo Li<br>
            <i>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2023</i> 
            <br>[<a href="https://arxiv.org/abs/2307.01831" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://dit-3d.github.io/" style="color:rgb(51, 156, 255)">project page</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/MaskIns.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Mask-Attention-Free Transformer for 3D Instance Segmentation</span></h5>
            Xin Lai, Yuhui Yuan, <b>Ruihang Chu</b>, Yukang Chen, Han Hu, Jiaya Jia<br>
            <i>IEEE International Conference on Computer Vision <b>(ICCV)</b>, 2023</i> 
            <br>[<a href="https://arxiv.org/pdf/2309.01692.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/Mask-Attention-Free-Transformer" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/TriVol.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">TriVol: Point Cloud Rendering via Triple Volumes</span></h5>
            Tao Hu, Xiaogang Xu, <b>Ruihang Chu</b>, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2023</i> 
            <br>[<a href="https://arxiv.org/pdf/2303.16485.pdf" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/dvlab-research/TriVol" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/Grasp.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Simultaneous Multi-task Learning for 6-DoF Grasp Pose Estimation</span></h5>
            Yiming Li, Tao Kong, <b>Ruihang Chu</b>, Yifeng Li, Peng Wang, Lei Li<br>
            <i>IEEE/RSJ International Conference on Intelligent Robots and Systems <b>(IROS)</b>, 2021</i>
            <br>[<a href="https://arxiv.org/abs/2108.02425" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://openbyterobotics.github.io/sscl/" style="color:rgb(51, 156, 255)">project</a>]
    </td></tr>
    <tr>
        <td class="pub_td1" style="width: 200px;"><img src="/img/publication/SA_AutoAug.png" class="papericon"></td>
        <td class="pub_td2" style="width: 600px;">
            <h5><span style="color:black">Scale-aware Automatic Augmentation for Object Detection</span></h5>
            Yukang Chen, Yanwei Li, Tao Kong, Lu Qi, <b>Ruihang Chu</b>, Lei Li, Jiaya Jia<br>
            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2021</i>
            <br>[<a href="https://arxiv.org/abs/2103.17220" style="color:rgb(51, 156, 255)">paper</a>] [<a href="https://github.com/Jia-Research-Lab/SA-AutoAug" style="color:rgb(51, 156, 255)">code</a>]
    </td></tr>
</table>
<hr>
